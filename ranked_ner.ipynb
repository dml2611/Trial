{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dml2611/Trial/blob/main/ranked_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBCPoZlRquhW"
      },
      "source": [
        "# **Install Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0av2d1PspOen"
      },
      "outputs": [],
      "source": [
        "# Install the following packages\n",
        "!pip uninstall tweepy\n",
        "!pip install tweepy\n",
        "!pip install num2words\n",
        "!pip install praw           # Python Reddit API Wrapper is a Python module that provides a simple access to Reddit's API\n",
        "!pip install svgling        # Draw NER Trees / SVG Diagrams / Rendering of linguistics-style constituent trees into SVG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMaYIJtorCYf"
      },
      "source": [
        "# **Reddit Scrapper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1EmTMTcCqMD6"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import praw\n",
        "\n",
        "def reddit_scrapper():\n",
        "  # Please change with your own client id, client secret, and user agent\n",
        "\n",
        "  # Initializing the client id, client secret, and user agent\n",
        "  reddit = praw.Reddit(client_id='NIrpLE8RCFTYCiHBocVL4w', \n",
        "                     client_secret='X1ymgznBjeU-H7-noZH4czW9PJ-8qA', \n",
        "                     user_agent='Scrapper', \n",
        "                     check_for_async = False)\n",
        "\n",
        "  # Get 5 hot posts from the CryptoCurrency subreddit\n",
        "  hot_posts = reddit.subreddit('CryptoCurrency').search('bitcoin', sort='top', limit=5)\n",
        "\n",
        "  # Fetching the reddits\n",
        "  reddit_posts = []\n",
        "  for post in hot_posts:\n",
        "    reddit_posts.append(post.selftext)\n",
        "  return reddit_posts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_jDA-UXrOhg"
      },
      "source": [
        "# **Twitter Scrapper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GUv6vOT8qauT"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import tweepy\n",
        "import re\n",
        "import pickle\n",
        "\n",
        "from tweepy import OAuthHandler\n",
        "\n",
        "def twitter_scrapper():\n",
        "  # Please change with your own consumer key, consumer secret, access token and access secret\n",
        "\n",
        "  # Initializing the keys\n",
        "  consumer_key = 'MpEJcCsElugUxpqtUbPKVfDja'\n",
        "  consumer_secret = 'GBcdBMpB2b0niN23oqrlgafjbpaC2lsWuRHem8Fn9Y6GgjWzB9' \n",
        "  access_token = '1353714375605936129-a4B9HUHw33z816KJnmJNfqrxNNHYel'\n",
        "  access_secret ='LmO1etlQQHPliSOAyzXBOsYkX9D8iScml2JOhhi0KcmHt'\n",
        "\n",
        "  # Initializing the tokens\n",
        "  auth = OAuthHandler(consumer_key, consumer_secret)\n",
        "  auth.set_access_token(access_token, access_secret)\n",
        "  args = ['bitcoin'];     \n",
        "  api = tweepy.API(auth,timeout=10)\n",
        "\n",
        "  # Fetching the tweets\n",
        "  twitter_posts = []\n",
        "  query = args[0]\n",
        "  if len(args) == 1:\n",
        "    for status in tweepy.Cursor(api.search_tweets, q=query+\" -filter:retweets\",lang='en',result_type='recent', tweet_mode = 'extended').items(5):\n",
        "      twitter_posts.append(status.full_text)\n",
        "  return twitter_posts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "xb2J4BMpFMTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import Libraries\n",
        "import re    \n",
        "from num2words import num2words     #To convert numbers into words in text\n",
        "\n",
        "#Function to remove emojis\n",
        "def remove_emoji(string):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', string)\n",
        "\n",
        "#Data Cleaning\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def data_preparation(scrapped_posts):\n",
        "  posts_filtered = []\n",
        "  for post in scrapped_posts:\n",
        "    post = re.sub(r\"^https://ibb.co/[a-zA-Z0-9]*\\s\", \" \", post)    #Remove links inside reddit posts\n",
        "    post = re.sub(r\"\\s+https://ibb.co/[a-zA-Z0-9]*\\s\", \" \", post)      \n",
        "    post = re.sub(r\"(https://ibb.co/[a-zA-Z0-9]*)\", \" \", post)          \n",
        "    post = re.sub(r\"\\s+https://ibb.co/[a-zA-Z0-9]*$\", \" \", post)        \n",
        "    post = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", post)      #Remove links inside twitter posts\n",
        "    post = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", post)         \n",
        "    post = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", post)\n",
        "    post = re.sub(\"@[A-Za-z0-9_]+\",\"\", post)      #Remove mentions\n",
        "    post = re.sub(\"#[A-Za-z0-9_]+\",\"\", post)         #Remove hashtags\n",
        "    post = re.compile(r'<[^>]+>').sub(\" \", post)        #Remove html tags\n",
        "    post = re.sub(r\"â€™\",\"'\", post)\n",
        "    post = re.sub(r\"that's\",\"that is\", post)\n",
        "    post = re.sub(r\"there's\",\"there is\", post)\n",
        "    post = re.sub(r\"what's\",\"what is\", post)\n",
        "    post = re.sub(r\"where's\",\"where is\", post)\n",
        "    post = re.sub(r\"it's\",\"it is\", post)\n",
        "    post = re.sub(r\"It's\",\"it is\", post)\n",
        "    post = re.sub(r\"I'm\",\"I am\", post)\n",
        "    post = re.sub(r\"who's\",\"who is\", post)\n",
        "    post = re.sub(r\"i'm\",\"i am\", post)\n",
        "    post = re.sub(r\"she's\",\"she is\", post)\n",
        "    post = re.sub(r\"he's\",\"he is\", post)\n",
        "    post = re.sub(r\"you're\",\"you are\", post)\n",
        "    post = re.sub(r\"they're\",\"they are\", post)\n",
        "    post = re.sub(r\"who're\",\"who are\", post)\n",
        "    post = re.sub(r\"ain't\",\"am not\", post)\n",
        "    post = re.sub(r\"wouldn't\",\"would not\", post)\n",
        "    post = re.sub(r\"shouldn't\",\"should not\", post)\n",
        "    post = re.sub(r\"can't\",\"can not\", post)\n",
        "    post = re.sub(r\"couldn't\",\"could not\", post)\n",
        "    post = re.sub(r\"won't\",\"will not\", post)\n",
        "    post = re.sub(r\"don't\",\"do not\", post)\n",
        "    post = re.sub(r\"%\",\" percent\", post)\n",
        "        #post = re.sub(r\"\\d\",\" \", post)        #Remove digits\n",
        "    post = re.sub(r\"\\s+[s]\\s+\",\" \", post)\n",
        "    post = re.sub(r\"\\s*[\\[\\]\\(\\)\\-@\\*#<>$\\'\\\":]\\s*\",\" \", post)     #Remove special symbols\n",
        "    post = re.sub(r\"\\s+\",\" \", post) #Remove special symbols\n",
        "    #words = [word for word in nltk.word_tokenize(post) if word.lower() not in stopwords.words('english')]        #Stopword Removal\n",
        "    #posts_filtered.append(' '.join(words))\n",
        "    posts_filtered.append(post)\n",
        "  return posts_filtered\n",
        "\n",
        "def filter_emojis(posts):\n",
        "  posts_filtered = []\n",
        "  for post in posts:\n",
        "    post = remove_emoji(post)\n",
        "    posts_filtered.append(post)\n",
        "  return posts_filtered\n",
        "\n",
        "def number_to_words(posts):\n",
        "  num2words_posts = []\n",
        "  for post in posts:\n",
        "    numbers = re.findall(r'\\d+', post)\n",
        "    for num in numbers:\n",
        "      post = re.sub(num, str(num2words(int(num))+\" \"), post)\n",
        "    num2words_posts.append(post)\n",
        "  return num2words_posts"
      ],
      "metadata": {
        "id": "WRP9lkCAFNuG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzvtIvDcrUzK"
      },
      "source": [
        "# **POS-Tagging using NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKwxsvMnrZcJ",
        "outputId": "eecbccdd-3d5f-410a-9eb6-6958d64b122c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# POS-Tagger NLTK\n",
        "def pos_tagger(posts):\n",
        "  nltk_tagged_sequences = []\n",
        "\n",
        "  for post in posts:\n",
        "    dictionary = {}\n",
        "    word_tags = []\n",
        "    word_tokens = nltk.word_tokenize(post)\n",
        "    tagged_words = nltk.pos_tag(word_tokens)\n",
        "    for tw in tagged_words:\n",
        "      word_tags.append((tw[0], tw[1]))\n",
        "    nltk_tagged_sequences.append(word_tags)\n",
        "  return nltk_tagged_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR-qo1Q3rynw"
      },
      "source": [
        "# **NER using NLTK** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "pOjIQIdMrnwP"
      },
      "outputs": [],
      "source": [
        "# Named Entity Extraction\n",
        "def entity_extraction_nltk(posts):\n",
        "  nltk_ner_list = []\n",
        "  draw_ner_tree_list = []\n",
        "  for i in range(len(posts)):\n",
        "    nltk_ner = []\n",
        "    for chunk in nltk.ne_chunk(posts[i]):\n",
        "      draw_ner_tree_list.append(nltk.ne_chunk(posts[i]))\n",
        "      if hasattr(chunk, 'label'):\n",
        "        nltk_ner.append((chunk.label(), ' '.join(c[0] for c in chunk)))\n",
        "    nltk_ner_list.append(nltk_ner)\n",
        "  return nltk_ner_list, draw_ner_tree_list\n",
        "\n",
        "\n",
        "#Ranked NER Tags\n",
        "def ranked_frequencies_nltk(nltk_ner_list):\n",
        "  nltk_frequency = []\n",
        "  nltk_ranked_frequency = []\n",
        "\n",
        "  for item in nltk_ner_list:\n",
        "    dictionary = {'PERSON': 0, 'ORGANIZATION': 0, 'FACILITY': 0, 'GSP': 0, 'LOCATION': 0, 'GPE': 0, 'DATE': 0, 'TIME': 0, 'DURATION' : 0, 'SET': 0, 'MONEY': 0, 'NUMBER': 0, 'ORDINAL': 0, 'PERCENT': 0}\n",
        "    for ner_tag in item:\n",
        "      if ner_tag[0] == 'PERSON':\n",
        "        dictionary['PERSON'] = dictionary['PERSON'] + 1\n",
        "      elif ner_tag[0] == 'ORGANIZATION':\n",
        "        dictionary['ORGANIZATION'] = dictionary['ORGANIZATION'] + 1\n",
        "      elif ner_tag[0] == 'FACILITY':\n",
        "        dictionary['FACILITY'] = dictionary['FACILITY'] + 1\n",
        "      elif ner_tag[0] == 'GSP':\n",
        "        dictionary['GSP'] = dictionary['GSP'] + 1\n",
        "      elif ner_tag[0] == 'LOCATION':\n",
        "        dictionary['LOCATION'] = dictionary['LOCATION'] + 1\n",
        "      elif ner_tag[0] == 'GPE':\n",
        "        dictionary['GPE'] = dictionary['GPE'] + 1\n",
        "      elif ner_tag[0] == 'DATE':\n",
        "        dictionary['DATE'] = dictionary['DATE'] + 1\n",
        "      elif ner_tag[0] == 'TIME':\n",
        "        dictionary['TIME'] = dictionary['TIME'] + 1\n",
        "      elif ner_tag[0] == 'DURATION':\n",
        "        dictionary['DURATION'] = dictionary['DURATION'] + 1\n",
        "      elif ner_tag[0] == 'SET':\n",
        "        dictionary['SET'] = dictionary['SET'] + 1\n",
        "      elif ner_tag[0] == 'MONEY':\n",
        "        dictionary['MONEY'] = dictionary['MONEY'] + 1\n",
        "      elif ner_tag[0] == 'NUMBER':\n",
        "        dictionary['NUMBER'] = dictionary['NUMBER'] + 1\n",
        "      elif ner_tag[0] == 'ORDINAL':\n",
        "        dictionary['ORDINAL'] = dictionary['ORDINAL'] + 1\n",
        "      elif ner_tag[0] == 'PERCENT':\n",
        "        dictionary['PERCENT'] = dictionary['PERCENT'] + 1\n",
        "\n",
        "    nltk_frequency.append(dictionary)\n",
        "\n",
        "    for dictionary in nltk_frequency:\n",
        "      nltk_ranked_frequency.append(sorted(dictionary.items(), key=lambda x: x[1], reverse=True))\n",
        "  return nltk_ranked_frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh6ORBS5r_9e"
      },
      "source": [
        "# **NER using Spacy** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "5U1z1HwEr_Hq"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "#Named Entity Extraction using Spacy\n",
        "def entity_extraction_spacy(posts):\n",
        "  spacy_ner_list = []\n",
        "  draw_ner_tree_list = []\n",
        "  for post in posts:\n",
        "    tagged_words = nlp(post)\n",
        "    draw_ner_tree_list.append( nlp(post))\n",
        "    spacy_ner_list.append([(X.text, X.label_) for X in tagged_words.ents])\n",
        "  return spacy_ner_list, draw_ner_tree_list\n",
        "\n",
        "# Ranked NER Tags\n",
        "def ranked_frequencies_spacy(spacy_ner_list):\n",
        "  spacy_frequency = []\n",
        "  spacy_ranked_frequency = []\n",
        "\n",
        "  for item in spacy_ner_list:\n",
        "    dictionary = {'PERSON': 0, 'ORGANIZATION': 0, 'FACILITY': 0, 'GSP': 0, 'LOCATION': 0, 'GPE': 0, 'DATE': 0, 'TIME': 0, 'DURATION' : 0, 'SET': 0, 'MONEY': 0, 'NUMBER': 0, 'ORDINAL': 0, 'PERCENT': 0}\n",
        "    for ner_tag in item:\n",
        "      if ner_tag[1] == 'PERSON':\n",
        "        dictionary['PERSON'] = dictionary['PERSON'] + 1\n",
        "      elif ner_tag[1] == 'ORGANIZATION':\n",
        "        dictionary['ORGANIZATION'] = dictionary['ORGANIZATION'] + 1\n",
        "      elif ner_tag[1] == 'FACILITY':\n",
        "        dictionary['FACILITY'] = dictionary['FACILITY'] + 1\n",
        "      elif ner_tag[1] == 'GSP':\n",
        "        dictionary['GSP'] = dictionary['GSP'] + 1\n",
        "      elif ner_tag[1] == 'LOCATION':\n",
        "        dictionary['LOCATION'] = dictionary['LOCATION'] + 1\n",
        "      elif ner_tag[1] == 'GPE':\n",
        "        dictionary['GPE'] = dictionary['GPE'] + 1\n",
        "      elif ner_tag[1] == 'DATE':\n",
        "        dictionary['DATE'] = dictionary['DATE'] + 1\n",
        "      elif ner_tag[1] == 'TIME':\n",
        "        dictionary['TIME'] = dictionary['TIME'] + 1\n",
        "      elif ner_tag[1] == 'DURATION':\n",
        "        dictionary['DURATION'] = dictionary['DURATION'] + 1\n",
        "      elif ner_tag[1] == 'SET':\n",
        "        dictionary['SET'] = dictionary['SET'] + 1\n",
        "      elif ner_tag[1] == 'MONEY':\n",
        "        dictionary['MONEY'] = dictionary['MONEY'] + 1\n",
        "      elif ner_tag[1] == 'NUMBER':\n",
        "        dictionary['NUMBER'] = dictionary['NUMBER'] + 1\n",
        "      elif ner_tag[1] == 'ORDINAL':\n",
        "        dictionary['ORDINAL'] = dictionary['ORDINAL'] + 1\n",
        "      elif ner_tag[1] == 'PERCENT':\n",
        "        dictionary['PERCENT'] = dictionary['PERCENT'] + 1\n",
        "\n",
        "    spacy_frequency.append(dictionary)\n",
        "\n",
        "    for dictionary in spacy_frequency:\n",
        "      spacy_ranked_frequency.append(sorted(dictionary.items(), key=lambda x: x[1], reverse=True))\n",
        "  return spacy_ranked_frequency\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape Twitter\n",
        "tweets = twitter_scrapper()\n",
        "print(\"TWEETS\")\n",
        "print(tweets)\n",
        "print()\n",
        "\n",
        "# Preprocess tweets for removing hashtags, mentions, html tags, emojis, etc.\n",
        "twitter_posts = data_preparation(tweets)\n",
        "twitter_posts = filter_emojis(twitter_posts)\n",
        "twitter_posts = number_to_words(twitter_posts)\n",
        "print(\"FILTERED TWEETS\")\n",
        "print(twitter_posts)\n",
        "print()\n",
        "\n",
        "# POS Tagging\n",
        "tagged_tweets = pos_tagger(twitter_posts)\n",
        "print(\"POS-TAGGED TWEETS\")\n",
        "print(tagged_tweets)\n",
        "print()\n",
        "\n",
        "# NER using NLTK\n",
        "ner_tweets_nltk, draw_ner_tree_nltk = entity_extraction_nltk(tagged_tweets)\n",
        "print(\"NER TAGS FOR TWEETS USING NLTK\")\n",
        "print(ner_tweets_nltk)\n",
        "print()\n",
        "\n",
        "ranked_ner_tweets_nltk = ranked_frequencies_nltk(ner_tweets_nltk)\n",
        "print(\"RANKED NER TAGS FOR TWEETS USING NLTK\")\n",
        "print(ranked_ner_tweets_nltk)\n",
        "print()\n",
        "\n",
        "# NER using Spacy\n",
        "ner_tweets_spacy, draw_ner_tree_spacy = entity_extraction_spacy(twitter_posts)\n",
        "print(\"NER TAGS FOR TWEETS USING SPACY\")\n",
        "print(ner_tweets_spacy)\n",
        "print()\n",
        "\n",
        "# Scrape Twitter\n",
        "tweets = twitter_scrapper()\n",
        "print(\"TWEETS\")\n",
        "print(tweets)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKE6kQExEEol",
        "outputId": "685e5436-741e-4491-91c1-6f11b2675d23"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TWEETS\n",
            "['Documentary: 48 HOURS in Amsterdam - No CASH, Only BITCOIN https://t.co/nVONYsCEDo via @YouTube', '#Bitcoin Last Price $19433 #BTC ðŸš€\\nDaily Indicators:\\nâ€¢Variation since 00h00(UTC): +1.19%\\nâ€¢MACD:\\n -MACD Line: -115\\n -Signal Line: -145\\n\\n#Ethereum Last Price $1329 #ETH\\nâ€¢Variation: +1.18%\\nâ€¢MACD:\\n -MACD Line: -20\\n -Signal Line: -29\\n\\n#Blockchain #Web3\\n(2/6)\\n303223', 'Beautiful destinations.\\nReservations and information in my bio.\\nhttps://t.co/3QAVh1UGI6\\n#travel #love #home #revolut #bitcoin  #instagood #style  #beautiful #fitness #picoftheday #follow #beautiful #followme  #happy  #girl #lifestyle https://t.co/BeGbqfGmeZ', 'Seen the announcement of that round 1 went live, lets go waagmi! @Przemek0505 @BowTiedBronco @FriscaCal @FoodYoujia @bumskinsx @loop_225 @m_ratsim @LTomosvari @NSEx78DqYM3tVyU @robin55661 @BITCOIN_AVAX @nkbelgin @Vijayanandk2k @Nstorsantos5 https://t.co/PgUvdOZV89', '#Stablecoins/Bitcoin chart. This is nearly a year timeframe going back to the #bitcoin peak around November â€˜01. We broke trend line down indicating the possible beginning of stable coin flow back into #btc/crypto https://t.co/lRJbc4BLd1']\n",
            "\n",
            "FILTERED TWEETS\n",
            "['Documentary forty-eight  HOURS in Amsterdam No CASH, Only BITCOIN via ', ' Last Price nineteen thousand, four hundred and thirty-three   Daily Indicators â€¢Variation since zero hzero  UTC +one .one 9 percent â€¢MACD MACD Line one one 5 Signal Line one 45 Last Price one 3twenty-nine  â€¢Variation +one .one 8 percent â€¢MACD MACD Line twenty  Signal Line twenty-nine  two /six  303two two 3', 'Beautiful destinations. Reservations and information in my bio. ', 'Seen the announcement of that round one  went live, lets go waagmi! ', '/Bitcoin chart. This is nearly a year timeframe going back to the peak around November â€˜one . We broke trend line down indicating the possible beginning of stable coin flow back into /crypto ']\n",
            "\n",
            "POS-TAGGED TWEETS\n",
            "[[('Documentary', 'JJ'), ('forty-eight', 'JJ'), ('HOURS', 'NNP'), ('in', 'IN'), ('Amsterdam', 'NNP'), ('No', 'NNP'), ('CASH', 'NNP'), (',', ','), ('Only', 'RB'), ('BITCOIN', 'NNP'), ('via', 'IN')], [('Last', 'JJ'), ('Price', 'NNP'), ('nineteen', 'NN'), ('thousand', 'CD'), (',', ','), ('four', 'CD'), ('hundred', 'VBD'), ('and', 'CC'), ('thirty-three', 'JJ'), ('Daily', 'NNP'), ('Indicators', 'NNPS'), ('â€¢Variation', 'NN'), ('since', 'IN'), ('zero', 'CD'), ('hzero', 'NN'), ('UTC', 'NNP'), ('+one', 'NNP'), ('.one', 'VBD'), ('9', 'CD'), ('percent', 'NN'), ('â€¢MACD', 'NNP'), ('MACD', 'NNP'), ('Line', 'NNP'), ('one', 'CD'), ('one', 'CD'), ('5', 'CD'), ('Signal', 'NNP'), ('Line', 'NNP'), ('one', 'CD'), ('45', 'CD'), ('Last', 'JJ'), ('Price', 'NNP'), ('one', 'CD'), ('3twenty-nine', 'JJ'), ('â€¢Variation', 'NN'), ('+one', 'NN'), ('.one', 'CD'), ('8', 'CD'), ('percent', 'NN'), ('â€¢MACD', 'NNP'), ('MACD', 'NNP'), ('Line', 'NNP'), ('twenty', 'IN'), ('Signal', 'NNP'), ('Line', 'NNP'), ('twenty-nine', 'JJ'), ('two', 'CD'), ('/six', '$'), ('303two', 'CD'), ('two', 'CD'), ('3', 'CD')], [('Beautiful', 'JJ'), ('destinations', 'NNS'), ('.', '.'), ('Reservations', 'NNS'), ('and', 'CC'), ('information', 'NN'), ('in', 'IN'), ('my', 'PRP$'), ('bio', 'NN'), ('.', '.')], [('Seen', 'NNP'), ('the', 'DT'), ('announcement', 'NN'), ('of', 'IN'), ('that', 'DT'), ('round', 'NN'), ('one', 'CD'), ('went', 'VBD'), ('live', 'JJ'), (',', ','), ('lets', 'NNS'), ('go', 'VBP'), ('waagmi', 'NN'), ('!', '.')], [('/Bitcoin', 'JJ'), ('chart', 'NN'), ('.', '.'), ('This', 'DT'), ('is', 'VBZ'), ('nearly', 'RB'), ('a', 'DT'), ('year', 'NN'), ('timeframe', 'NN'), ('going', 'VBG'), ('back', 'RB'), ('to', 'TO'), ('the', 'DT'), ('peak', 'NN'), ('around', 'IN'), ('November', 'NNP'), ('â€˜', 'NNP'), ('one', 'NN'), ('.', '.'), ('We', 'PRP'), ('broke', 'VBD'), ('trend', 'NN'), ('line', 'NN'), ('down', 'IN'), ('indicating', 'VBG'), ('the', 'DT'), ('possible', 'JJ'), ('beginning', 'NN'), ('of', 'IN'), ('stable', 'JJ'), ('coin', 'NN'), ('flow', 'NN'), ('back', 'RB'), ('into', 'IN'), ('/crypto', 'NN')]]\n",
            "\n",
            "NER TAGS FOR TWEETS USING NLTK\n",
            "[[('ORGANIZATION', 'HOURS'), ('GPE', 'Amsterdam'), ('ORGANIZATION', 'BITCOIN')], [('ORGANIZATION', 'Daily Indicators'), ('ORGANIZATION', 'UTC'), ('ORGANIZATION', 'MACD'), ('ORGANIZATION', 'MACD Line'), ('ORGANIZATION', 'Signal Line')], [('GPE', 'Beautiful')], [], []]\n",
            "\n",
            "RANKED NER TAGS FOR TWEETS USING NLTK\n",
            "[[('ORGANIZATION', 2), ('GPE', 1), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 2), ('GPE', 1), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 5), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 2), ('GPE', 1), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 5), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('GPE', 1), ('PERSON', 0), ('ORGANIZATION', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 2), ('GPE', 1), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 5), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('GPE', 1), ('PERSON', 0), ('ORGANIZATION', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('PERSON', 0), ('ORGANIZATION', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 2), ('GPE', 1), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('ORGANIZATION', 5), ('PERSON', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('GPE', 1), ('PERSON', 0), ('ORGANIZATION', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('PERSON', 0), ('ORGANIZATION', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)], [('PERSON', 0), ('ORGANIZATION', 0), ('FACILITY', 0), ('GSP', 0), ('LOCATION', 0), ('GPE', 0), ('DATE', 0), ('TIME', 0), ('DURATION', 0), ('SET', 0), ('MONEY', 0), ('NUMBER', 0), ('ORDINAL', 0), ('PERCENT', 0)]]\n",
            "\n",
            "NER TAGS FOR TWEETS USING SPACY\n",
            "[[('forty-eight', 'CARDINAL'), ('Amsterdam', 'GPE')], [('nineteen thousand', 'DATE'), ('four hundred and thirty-three', 'CARDINAL'), ('zero', 'CARDINAL'), ('UTC', 'ORG'), ('9 percent', 'PERCENT'), ('one', 'CARDINAL'), ('5', 'CARDINAL'), ('Signal Line', 'ORG'), ('one', 'CARDINAL'), ('one', 'CARDINAL'), ('3twenty-nine', 'CARDINAL'), ('8 percent', 'PERCENT'), ('twenty', 'CARDINAL'), ('Signal Line', 'ORG'), ('twenty-nine', 'CARDINAL'), ('303two', 'CARDINAL'), ('3', 'CARDINAL')], [], [], [('nearly a year', 'DATE')]]\n",
            "\n",
            "TWEETS\n",
            "['Documentary: 48 HOURS in Amsterdam - No CASH, Only BITCOIN https://t.co/nVONYsCEDo via @YouTube', '#Bitcoin Last Price $19433 #BTC ðŸš€\\nDaily Indicators:\\nâ€¢Variation since 00h00(UTC): +1.19%\\nâ€¢MACD:\\n -MACD Line: -115\\n -Signal Line: -145\\n\\n#Ethereum Last Price $1329 #ETH\\nâ€¢Variation: +1.18%\\nâ€¢MACD:\\n -MACD Line: -20\\n -Signal Line: -29\\n\\n#Blockchain #Web3\\n(2/6)\\n303223', 'Beautiful destinations.\\nReservations and information in my bio.\\nhttps://t.co/3QAVh1UGI6\\n#travel #love #home #revolut #bitcoin  #instagood #style  #beautiful #fitness #picoftheday #follow #beautiful #followme  #happy  #girl #lifestyle https://t.co/BeGbqfGmeZ', 'Seen the announcement of that round 1 went live, lets go waagmi! @Przemek0505 @BowTiedBronco @FriscaCal @FoodYoujia @bumskinsx @loop_225 @m_ratsim @LTomosvari @NSEx78DqYM3tVyU @robin55661 @BITCOIN_AVAX @nkbelgin @Vijayanandk2k @Nstorsantos5 https://t.co/PgUvdOZV89', '#Stablecoins/Bitcoin chart. This is nearly a year timeframe going back to the #bitcoin peak around November â€˜01. We broke trend line down indicating the possible beginning of stable coin flow back into #btc/crypto https://t.co/lRJbc4BLd1']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_CAI8W9dLaDd"
      },
      "execution_count": 60,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYySJghal8kHVxLYLsq26L",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}